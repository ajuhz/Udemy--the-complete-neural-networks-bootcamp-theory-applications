{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical NN using Pytorch : Digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "#import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "#import pandas as pd\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from torch.utils.data import Dataset\n",
    "from torchvision import datasets \n",
    "import torchvision.transforms as transforms # to transform the dataset into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f71351c15b645f2b13d8ac60ba8be8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b64e754f02a4a5d8de726c1ef175eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7266a9e1147140e1b34dce20a4cb6ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a374ef1d52647c7b69ab7a3642c86b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# download the training and test dataset in local folder\n",
    "train_dataset = datasets.MNIST(root='./',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 85% training dataset and 15% test dataset\n",
    "print(train_dataset.data.shape) # we have 60K sample of 28x28 because it is having 28x28 pixels\n",
    "print(test_dataset.data.shape) # we have 10K sample of 28x28 because it is having 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =100\n",
    "# Create dataloader for test and train dataset\n",
    "# Load the data to your dataloader for batch processing and shuffling\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netowrk parameters\n",
    "# As specified above we have input_neurons = 784 (28*28) \n",
    "# hidden layer neurons = 400 (apx... (input_neurons+output_neurons/2))\n",
    "# output neurons = 10 (#no of labels)\n",
    "input_size = 784        #Number of input neurons (image pixels)\n",
    "hidden_size = 400       #Number of hidden neurons\n",
    "out_size = 10           #Number of classes (0-9) \n",
    "epochs = 10            #How many times we pass our entire dataset into our network \n",
    "batch_size = 100        #Input size of the data during one iteration \n",
    "learning_rate = 0.001   #How fast we are learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining NN Architecture \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,out_size):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,hidden_size)\n",
    "        self.fc2=nn.Linear(hidden_size,hidden_size)\n",
    "        self.fc3=nn.Linear(hidden_size,out_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.init_weights() # Initalizing weights\n",
    "    def init_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight) # He initialization\n",
    "        nn.init.kaiming_normal_(self.fc2.weight) # He initialization\n",
    "    def forward(self,x):\n",
    "        out=self.fc1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        out=self.relu(out)\n",
    "        out= self.fc3(out)\n",
    "        return out       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object of network class and defining loss and optimizer\n",
    "net=Net(input_size,hidden_size,out_size)\n",
    "#The loss function. The Cross Entropy loss comes along with Softmax. Therefore, no need to specify Softmax as well\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.002, Training Accuracy: 99.920%\n",
      "Epoch [1/10], Training Loss: 0.000, Training Accuracy: 99.930%\n",
      "Epoch [1/10], Training Loss: 0.000, Training Accuracy: 99.897%\n",
      "Epoch [1/10], Training Loss: 0.000, Training Accuracy: 99.835%\n",
      "Epoch [1/10], Training Loss: 0.000, Training Accuracy: 99.826%\n",
      "Epoch [1/10], Training Loss: 0.003, Training Accuracy: 99.800%\n",
      "Epoch [2/10], Training Loss: 0.000, Training Accuracy: 99.810%\n",
      "Epoch [2/10], Training Loss: 0.001, Training Accuracy: 99.660%\n",
      "Epoch [2/10], Training Loss: 0.001, Training Accuracy: 99.633%\n",
      "Epoch [2/10], Training Loss: 0.003, Training Accuracy: 99.588%\n",
      "Epoch [2/10], Training Loss: 0.001, Training Accuracy: 99.604%\n",
      "Epoch [2/10], Training Loss: 0.000, Training Accuracy: 99.580%\n",
      "Epoch [3/10], Training Loss: 0.014, Training Accuracy: 99.840%\n",
      "Epoch [3/10], Training Loss: 0.000, Training Accuracy: 99.855%\n",
      "Epoch [3/10], Training Loss: 0.026, Training Accuracy: 99.840%\n",
      "Epoch [3/10], Training Loss: 0.002, Training Accuracy: 99.843%\n",
      "Epoch [3/10], Training Loss: 0.006, Training Accuracy: 99.852%\n",
      "Epoch [3/10], Training Loss: 0.000, Training Accuracy: 99.858%\n",
      "Epoch [4/10], Training Loss: 0.001, Training Accuracy: 99.900%\n",
      "Epoch [4/10], Training Loss: 0.000, Training Accuracy: 99.945%\n",
      "Epoch [4/10], Training Loss: 0.000, Training Accuracy: 99.947%\n",
      "Epoch [4/10], Training Loss: 0.000, Training Accuracy: 99.918%\n",
      "Epoch [4/10], Training Loss: 0.000, Training Accuracy: 99.864%\n",
      "Epoch [4/10], Training Loss: 0.001, Training Accuracy: 99.832%\n",
      "Epoch [5/10], Training Loss: 0.000, Training Accuracy: 99.770%\n",
      "Epoch [5/10], Training Loss: 0.002, Training Accuracy: 99.740%\n",
      "Epoch [5/10], Training Loss: 0.004, Training Accuracy: 99.743%\n",
      "Epoch [5/10], Training Loss: 0.058, Training Accuracy: 99.703%\n",
      "Epoch [5/10], Training Loss: 0.000, Training Accuracy: 99.676%\n",
      "Epoch [5/10], Training Loss: 0.000, Training Accuracy: 99.675%\n",
      "Epoch [6/10], Training Loss: 0.021, Training Accuracy: 99.820%\n",
      "Epoch [6/10], Training Loss: 0.000, Training Accuracy: 99.830%\n",
      "Epoch [6/10], Training Loss: 0.000, Training Accuracy: 99.857%\n",
      "Epoch [6/10], Training Loss: 0.001, Training Accuracy: 99.845%\n",
      "Epoch [6/10], Training Loss: 0.000, Training Accuracy: 99.852%\n",
      "Epoch [6/10], Training Loss: 0.003, Training Accuracy: 99.848%\n",
      "Epoch [7/10], Training Loss: 0.000, Training Accuracy: 99.780%\n",
      "Epoch [7/10], Training Loss: 0.001, Training Accuracy: 99.795%\n",
      "Epoch [7/10], Training Loss: 0.003, Training Accuracy: 99.783%\n",
      "Epoch [7/10], Training Loss: 0.000, Training Accuracy: 99.767%\n",
      "Epoch [7/10], Training Loss: 0.022, Training Accuracy: 99.744%\n",
      "Epoch [7/10], Training Loss: 0.000, Training Accuracy: 99.738%\n",
      "Epoch [8/10], Training Loss: 0.000, Training Accuracy: 99.730%\n",
      "Epoch [8/10], Training Loss: 0.000, Training Accuracy: 99.755%\n",
      "Epoch [8/10], Training Loss: 0.000, Training Accuracy: 99.750%\n",
      "Epoch [8/10], Training Loss: 0.001, Training Accuracy: 99.763%\n",
      "Epoch [8/10], Training Loss: 0.001, Training Accuracy: 99.768%\n",
      "Epoch [8/10], Training Loss: 0.004, Training Accuracy: 99.753%\n",
      "Epoch [9/10], Training Loss: 0.000, Training Accuracy: 99.680%\n",
      "Epoch [9/10], Training Loss: 0.006, Training Accuracy: 99.735%\n",
      "Epoch [9/10], Training Loss: 0.002, Training Accuracy: 99.760%\n",
      "Epoch [9/10], Training Loss: 0.042, Training Accuracy: 99.752%\n",
      "Epoch [9/10], Training Loss: 0.003, Training Accuracy: 99.750%\n",
      "Epoch [9/10], Training Loss: 0.006, Training Accuracy: 99.743%\n",
      "Epoch [10/10], Training Loss: 0.000, Training Accuracy: 99.890%\n",
      "Epoch [10/10], Training Loss: 0.003, Training Accuracy: 99.940%\n",
      "Epoch [10/10], Training Loss: 0.000, Training Accuracy: 99.940%\n",
      "Epoch [10/10], Training Loss: 0.000, Training Accuracy: 99.942%\n",
      "Epoch [10/10], Training Loss: 0.000, Training Accuracy: 99.930%\n",
      "Epoch [10/10], Training Loss: 0.001, Training Accuracy: 99.908%\n",
      "DONE TRAINING!\n"
     ]
    }
   ],
   "source": [
    "#Train the network\n",
    "for epoch in range(epochs):\n",
    "    correct_train = 0\n",
    "    running_loss = 0\n",
    "    total_train =0\n",
    "    for i,[images,labels] in enumerate(train_loader):\n",
    "        images=images.view(-1,28*28)\n",
    "        output=net(images)\n",
    "        _,predicted = torch.max(output.data,1)\n",
    "        correct_train += (predicted==labels).sum()\n",
    "        loss = criterion(output,labels)\n",
    "        running_loss+=loss.item()\n",
    "        total_train+=labels.size(0)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()                                   # Backpropagation\n",
    "        optimizer.step()  # How the optimizer and loss is linked?\n",
    "        if (i+1)%100==0:\n",
    "            print('Epoch [{}/{}], Training Loss: {:.3f}, Training Accuracy: {:.3f}%'.format\n",
    "          (epoch+1, epochs, loss.item(), (100*correct_train.double()/total_train)))\n",
    "print(\"DONE TRAINING!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.84 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step#:~:text=After%20computing%20the%20gradients%20for,grad%20to%20update%20their%20values.\n",
    "\n",
    "\n",
    "Without delving too deep into the internals of pytorch, I can offer a simplistic answer:\n",
    "\n",
    "Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values.\n",
    "\n",
    "example:\n",
    "\n",
    "# Our \"model\"\n",
    "x = torch.tensor([1., 2.], requires_grad=True)\n",
    "y = 100*x\n",
    "\n",
    "# Compute loss\n",
    "loss = y.sum()\n",
    "\n",
    "# Compute gradients of the parameters w.r.t. the loss\n",
    "print(x.grad)     # None\n",
    "loss.backward()      \n",
    "print(x.grad)     # tensor([100., 100.])\n",
    "\n",
    "# MOdify the parameters by subtracting the gradient\n",
    "optim = torch.optim.SGD([x], lr=0.001)\n",
    "print(x)        # tensor([1., 2.], requires_grad=True)\n",
    "optim.step()\n",
    "print(x)        # tensor([0.9000, 1.9000], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

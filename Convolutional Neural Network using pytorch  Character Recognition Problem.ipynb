{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network using pytorch : Character Recognition Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn # because we will use module package to define the CNN\n",
    "import torchvision.transforms as transforms # because we will use to_tensor to convert our image to torch tensor\n",
    "import torchvision.datasets as datasets  # we will use MNIST dataset\n",
    "from torch.autograd import Variable  # we will wrap dataset values / not required from Pytorch version 0.4\n",
    "import torch.utils.data as data # to user DataLoader utility to make dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Donloading the test and train datasets from pytorch library\n",
    "#converting it to tensor and normaizing it\n",
    "mean_gray = 0.1307  # ?? how to derive this value\n",
    "stddev_gray = 0.3081 # ?? how to derive this value\n",
    "\n",
    "#Transform the images to tensors\n",
    "#Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) \n",
    "#for n channels, this transform will normalize each channel of the input torch.Tensor\n",
    "#i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "# Compose :Composes several transforms together.\n",
    "# Normalize : we need to pass mean and std for each channel, \n",
    "#as we are using grey scale image so we are passing here only one value for mean and std\n",
    "# earlier cases we were using sklearn to download the MNIST dataset, here we are using torchvision\n",
    "transforms = transforms.Compose([transforms.ToTensor (),transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
    "\n",
    "#load datasets\n",
    "train_dataset = datasets.MNIST(root='./',\n",
    "                               train=True,\n",
    "                               download=True,\n",
    "                               transform=transforms\n",
    "                               )\n",
    "test_dataset = datasets.MNIST(root='./',\n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data : tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
      "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
      "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
      "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
      "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
      "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
      "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
      "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
      "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
      "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
      "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
      "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
      "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
      "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
      "       dtype=torch.uint8)\n",
      "original data : 5\n",
      "Transformed Data :  [[[-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.38602826 -0.19510475 -0.19510475\n",
      "   -0.19510475  1.1795444   1.306827    1.803228   -0.09327888\n",
      "    1.688674    2.8214867   2.7196608   1.1922727  -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.04236595  0.03400347\n",
      "    0.77224106  1.5359352   1.7395868   2.7960303   2.7960303\n",
      "    2.7960303   2.7960303   2.7960303   2.4396398   1.7650434\n",
      "    2.7960303   2.6560197   2.0577927   0.39039403 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296  0.1994705   2.6051068   2.7960303\n",
      "    2.7960303   2.7960303   2.7960303   2.7960303   2.7960303\n",
      "    2.7960303   2.7960303   2.7705739   0.7595128   0.61950225\n",
      "    0.61950225  0.28856814  0.07218818 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.19510475  2.3632703   2.7960303\n",
      "    2.7960303   2.7960303   2.7960303   2.7960303   2.0959773\n",
      "    1.8923256   2.7196608   2.6432915  -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296  0.59404576  1.5613916\n",
      "    0.937708    2.7960303   2.7960303   2.185075   -0.2842024\n",
      "   -0.42421296  0.12310111  1.5359352  -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.2460177\n",
      "   -0.41148472  1.5359352   2.7960303   0.7213281  -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296  1.3450117   2.7960303   1.9941516  -0.3987565\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.2842024   1.9941516   2.7960303   0.46676344\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296  0.02127524  2.6432915   2.4396398\n",
      "    1.6123046   0.95043623 -0.41148472 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296  0.60677403  2.6305633\n",
      "    2.7960303   2.7960303   1.0904468  -0.10600711 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.14855757\n",
      "    1.9432386   2.7960303   2.7960303   1.4850222  -0.08055065\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.2205612   0.7595128   2.783302    2.7960303   1.9559668\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296  2.7451172   2.7960303   2.7451172\n",
      "    0.39039403 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296  0.1612858\n",
      "    1.2304575   1.905054    2.7960303   2.7960303   2.2105315\n",
      "   -0.3987565  -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296  0.07218818  1.4595658   2.4905527\n",
      "    2.7960303   2.7960303   2.7960303   2.7578456   1.8923256\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.11873534  1.0268056   2.3887267   2.7960303   2.7960303\n",
      "    2.7960303   2.7960303   2.1341622   0.5685893  -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.13146357  0.41585052\n",
      "    2.286901    2.7960303   2.7960303   2.7960303   2.7960303\n",
      "    2.0959773   0.60677403 -0.3987565  -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.19510475  1.7523152   2.3632703   2.7960303\n",
      "    2.7960303   2.7960303   2.7960303   2.0577927   0.59404576\n",
      "   -0.30965886 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296  0.2758399\n",
      "    1.7650434   2.452368    2.7960303   2.7960303   2.7960303\n",
      "    2.7960303   2.681476    1.2686423  -0.2842024  -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296  1.306827\n",
      "    2.7960303   2.7960303   2.7960303   2.2741728   1.2940987\n",
      "    1.2559141  -0.2205612  -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]\n",
      "  [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "   -0.42421296 -0.42421296 -0.42421296]]]\n",
      "Transformed Data :  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e156de8f88>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See the impact of the transformation we can run this once without transformation and\n",
    "# again after transformation to see how original data has difference w.r.t transformed data\n",
    "# This item is open need further research on this\n",
    "print('original data :',train_dataset.train_data[0])\n",
    "print('original data :',train_dataset.train_labels[0].item())\n",
    "print('Transformed Data : ' ,train_dataset[0][0].numpy())\n",
    "print('Transformed Data : ' ,train_dataset[0][1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "random_image = train_dataset[0][0].numpy()\n",
    "plt.imshow(random_image.reshape(28, 28), cmap='gray')\n",
    "print(train_dataset[0][1])   #Print the corresponding label for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dataset iterable\n",
    "batch_size = 100\n",
    "\n",
    "train_load=data.DataLoader(dataset=train_dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True)\n",
    "test_load=data.DataLoader(dataset=test_dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 images in the training set\n",
      "There are 10000 images in the test set\n",
      "There are 600 batches in the train loader\n",
      "There are 100 batches in the testloader\n"
     ]
    }
   ],
   "source": [
    "print('There are {} images in the training set'.format(len(train_dataset)))\n",
    "print('There are {} images in the test set'.format(len(test_dataset)))\n",
    "print('There are {} batches in the train loader'.format(len(train_load)))\n",
    "print('There are {} batches in the testloader'.format(len(test_load)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn mnist](https://user-images.githubusercontent.com/30661597/61713471-3c957d00-ad8b-11e9-9a38-e3f4d1e72565.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        #Same Padding = [(filter size - 1) / 2] (Same Padding--> input size = output size)\n",
    "        # How we are deriving that 8 filters needed/ filter size should be 3x3?\n",
    "        # there is no stated rule, normally everyone try to follow the state-of-the-art architecture\n",
    "        # for kernal size 3x3 and 5x5 is advised in many tutorials\n",
    "        # for grey scale image channel is 1 and for RGB 3\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
    "        #The output size of each of the 8 feature maps is \n",
    "        #[(input_size - filter_size + 2(padding) / stride) +1] --> [(28-3+2(1)/1)+1] = 28 (padding type is same)\n",
    "        #Batch normalization\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8) # passing no of filters\n",
    "        #RELU\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2) # i.e 2x2\n",
    "        #After max pooling, the output of each feature map is now 28/2 = 14\n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5,stride=1, padding=2)\n",
    "        #[(input_size - filter_size + 2(padding) / stride) +1] --> [(14-5+2*(2)/1)+1] = 14 (same padding)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        #After max pooling, the output of each feature map is 14/2 = 7\n",
    "        #Flatten the feature maps. You have 32 feature maps, each of them is of size 7x7 --> 32*7*7 = 1568\n",
    "        self.fc1 = nn.Linear(in_features=1568, out_features=600)\n",
    "        self.droput = nn.Dropout(p=0.5) #50% neurons will be dropped during training phase\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.cnn1(x)\n",
    "        out=self.batchnorm1(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.maxpool(out)\n",
    "        out=self.cnn2(out)\n",
    "        out=self.batchnorm2(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.maxpool(out)\n",
    "        #Now we have to flatten the output. This is where we apply the feed forward neural network as learned before! \n",
    "        #It will take the shape (batch_size, 1568) = (100, 1568)\n",
    "        out=out.view(-1,1568)\n",
    "        out=self.fc1(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.droput(out)\n",
    "        out=self.fc2(out)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN()\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For one iteration, this is what happens:\n",
      "Input Shape: torch.Size([100, 1, 28, 28])\n",
      "Labels Shape: torch.Size([100])\n",
      "Outputs Shape torch.Size([100, 10])\n",
      "Predicted Shape torch.Size([100])\n",
      "Predicted Tensor:\n",
      "tensor([8, 0, 1, 8, 0, 0, 8, 4, 0, 0, 2, 0, 0, 0, 8, 8, 8, 3, 8, 0, 8, 4, 1, 0,\n",
      "        8, 2, 8, 8, 1, 0, 1, 5, 8, 5, 8, 3, 3, 1, 0, 1, 3, 8, 0, 8, 0, 2, 8, 0,\n",
      "        8, 3, 1, 8, 8, 0, 5, 3, 0, 0, 3, 0, 0, 2, 3, 1, 8, 0, 0, 0, 8, 8, 0, 5,\n",
      "        1, 5, 3, 0, 8, 8, 8, 0, 0, 5, 1, 0, 0, 1, 1, 3, 0, 8, 7, 8, 8, 1, 0, 0,\n",
      "        5, 0, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "#Understand what's happening\n",
    "iteration = 0\n",
    "correct = 0\n",
    "\n",
    "for i,(inputs,labels) in enumerate (train_load):\n",
    "\n",
    "        \n",
    "    print(\"For one iteration, this is what happens:\")\n",
    "    print(\"Input Shape:\",inputs.shape)\n",
    "    print(\"Labels Shape:\",labels.shape)\n",
    "    output = model(inputs)\n",
    "    print(\"Outputs Shape\",output.shape)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(\"Predicted Shape\",predicted.shape)\n",
    "    print(\"Predicted Tensor:\")\n",
    "    print(predicted)\n",
    "    correct += (predicted == labels).sum()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Training Loss: 0.401, Training Accuracy: 89.000\n",
      "Epoch 2/25, Training Loss: 0.157, Training Accuracy: 95.000\n",
      "Epoch 3/25, Training Loss: 0.115, Training Accuracy: 96.000\n",
      "Epoch 4/25, Training Loss: 0.094, Training Accuracy: 97.000\n",
      "Epoch 5/25, Training Loss: 0.080, Training Accuracy: 97.000\n",
      "Epoch 6/25, Training Loss: 0.073, Training Accuracy: 97.000\n",
      "Epoch 7/25, Training Loss: 0.066, Training Accuracy: 98.000\n",
      "Epoch 8/25, Training Loss: 0.060, Training Accuracy: 98.000\n",
      "Epoch 9/25, Training Loss: 0.055, Training Accuracy: 98.000\n",
      "Epoch 10/25, Training Loss: 0.053, Training Accuracy: 98.000\n",
      "Epoch 11/25, Training Loss: 0.050, Training Accuracy: 98.000\n",
      "Epoch 12/25, Training Loss: 0.047, Training Accuracy: 98.000\n",
      "Epoch 13/25, Training Loss: 0.043, Training Accuracy: 98.000\n",
      "Epoch 14/25, Training Loss: 0.043, Training Accuracy: 98.000\n",
      "Epoch 15/25, Training Loss: 0.039, Training Accuracy: 98.000\n",
      "Epoch 16/25, Training Loss: 0.038, Training Accuracy: 98.000\n",
      "Epoch 17/25, Training Loss: 0.036, Training Accuracy: 98.000\n",
      "Epoch 18/25, Training Loss: 0.035, Training Accuracy: 98.000\n",
      "Epoch 19/25, Training Loss: 0.033, Training Accuracy: 99.000\n",
      "Epoch 20/25, Training Loss: 0.031, Training Accuracy: 99.000\n",
      "Epoch 21/25, Training Loss: 0.031, Training Accuracy: 99.000\n",
      "Epoch 22/25, Training Loss: 0.030, Training Accuracy: 99.000\n",
      "Epoch 23/25, Training Loss: 0.028, Training Accuracy: 99.000\n",
      "Epoch 24/25, Training Loss: 0.028, Training Accuracy: 99.000\n",
      "Epoch 25/25, Training Loss: 0.026, Training Accuracy: 99.000\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "num_epochs = 25\n",
    "\n",
    "#list to hold loss and accuracy \n",
    "\n",
    "train_loss=[]\n",
    "train_accuracy=[]\n",
    "test_loss=[]\n",
    "test_accuracy=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Reset these below variables to 0 at the begining of every epoch\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0\n",
    "    model.train()\n",
    "    for i,(inputs,labels) in enumerate(train_load):\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_fn(outputs,labels)\n",
    "        iter_loss+=loss.item()\n",
    "        optimizer.zero_grad()            # Clear off the gradient in (w = w - gradient)\n",
    "        loss.backward()                 # Backpropagation \n",
    "        optimizer.step()                # Update the weights\n",
    "        \n",
    "        #record the correct prediction \n",
    "        _,predicted=torch.max(outputs,1)\n",
    "        correct += (predicted==labels).sum()\n",
    "        iterations+=1\n",
    "        # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / len(train_dataset)))\n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}'\n",
    "           .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/25, Test Loss 13.416, Test Accuracy 10.000\n",
      "Epoch : 2/25, Test Loss 13.416, Test Accuracy 10.000\n",
      "Epoch : 3/25, Test Loss 13.416, Test Accuracy 10.000\n"
     ]
    }
   ],
   "source": [
    "#Testing the Model\n",
    "for epoch in range(3):\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    testing_loss = 0.0\n",
    "    model.eval()\n",
    "    for i, (inputs, labels) in enumerate(test_load):\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_fn(outputs,labels)\n",
    "        testing_loss += loss.item()\n",
    "        _,predicted=torch.max(outputs,1)\n",
    "        correct+=(predicted==labels).sum()\n",
    "        iterations+=1\n",
    "    test_loss.append(testing_loss/iterations)\n",
    "    test_accuracy.append((100 * correct / len(test_dataset)))\n",
    "    print('Epoch : {}/{}, Test Loss {:.3f}, Test Accuracy {:.3f}'.format(epoch+1,num_epochs,test_loss[-1],test_accuracy[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Training Loss: 0.026, Training Accuracy: 99.000, Testing Loss: 0.028, Testing Acc: 99.000\n",
      "Epoch 2/25, Training Loss: 0.026, Training Accuracy: 99.000, Testing Loss: 0.028, Testing Acc: 99.000\n",
      "Epoch 3/25, Training Loss: 0.026, Training Accuracy: 99.000, Testing Loss: 0.028, Testing Acc: 99.000\n",
      "Epoch 4/25, Training Loss: 0.026, Training Accuracy: 99.000, Testing Loss: 0.028, Testing Acc: 99.000\n",
      "Epoch 5/25, Training Loss: 0.026, Training Accuracy: 99.000, Testing Loss: 0.028, Testing Acc: 99.000\n"
     ]
    }
   ],
   "source": [
    "  #Testing\n",
    "    # here why we are printing train accuracy beside test accuracy is not clear\n",
    "    # this type of evolution is applicable if we are passing teh network through test batch after each epoch of train\n",
    "for epoch in range(5): \n",
    "    testing_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluation modeso that dropout will not be applied\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(test_load):\n",
    "\n",
    "\n",
    "\n",
    "            outputs = model(inputs)     \n",
    "            loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "            testing_loss += loss.item()\n",
    "            # Record the correct predictions for training data\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "            iterations += 1\n",
    "\n",
    "        # Record the Testing loss\n",
    "    test_loss.append(testing_loss/iterations)\n",
    "        # Record the Testing accuracy\n",
    "    test_accuracy.append((100 * correct / len(test_dataset)))\n",
    "\n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}'\n",
    "               .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "                 test_loss[-1], test_accuracy[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
